{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnRWBbJEw6NE"
      },
      "source": [
        "https://levelup.gitconnected.com/dqn-from-scratch-with-tensorflow-2-eb0541151049\n",
        "\n",
        "https://towardsdatascience.com/solving-lunar-lander-openaigym-reinforcement-learning-785675066197\n",
        "\n",
        "\n",
        "https://github.com/ranjitation/DQN-for-LunarLander/blob/master/dqn_agent.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rq_4mIVB1JE5"
      },
      "source": [
        "Action space(discrete)\n",
        "0-do nothing\n",
        "1-fire left engine\n",
        "2-fire down engine\n",
        "3-fire right engine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktM_sMpRIWF5"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpp1Ya1F4foy"
      },
      "source": [
        "# %tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEH7bq8KP_rV"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import numpy as np\n",
        "import gym\n",
        "import time\n",
        "from collections import deque\n",
        "import random\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc94M9r78CYB"
      },
      "source": [
        "!apt-get install ffmpeg freeglut3-dev xvfb;  # For visualization\n",
        "\n",
        "!pip install box2d-py;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbp7cgbruUxA"
      },
      "source": [
        "class DQNAgent():\n",
        "  \"\"\"\n",
        "  Deep Q-Network agent\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  actionSpace: int\n",
        "    Size of action space (4)\n",
        "  \n",
        "  stateSpace: int\n",
        "    Size of state space (8)\n",
        "\n",
        "  DQN: <class 'tensorflow.python.keras.engine.sequential.Sequential'>\n",
        "    Online DQN\n",
        "\n",
        "  targetDQN: <class 'tensorflow.python.keras.engine.sequential.Sequential'>\n",
        "    Target DQN\n",
        "\n",
        "  bestDQN: <class 'tensorflow.python.keras.engine.sequential.Sequential'>\n",
        "    Best model up to the current step in training\n",
        "\n",
        "  epsilon: float\n",
        "    Exploration factor\n",
        "  \n",
        "  TAU: float\n",
        "    Soft update factor\n",
        "\n",
        "  gamma: float\n",
        "    Discount rate\n",
        "\n",
        "  learningRate: float\n",
        "    Training learning rate\n",
        "\n",
        "  Methods\n",
        "  -------\n",
        "  policy(state)\n",
        "    The agent policy. \n",
        "  \"\"\"\n",
        "  def __init__(self,actionSpace,stateSpace):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    actionSpace: int\n",
        "    Size of action space (4)\n",
        "  \n",
        "    stateSpace: int\n",
        "      Size of state space (8)\n",
        "    \"\"\"\n",
        "    self.actionSpace = actionSpace\n",
        "    self.stateSpace = stateSpace\n",
        "    \n",
        "    self.epsilon = 0.05\n",
        "    self.TAU = 0.001\n",
        "    self.gamma = 0.99\n",
        "    self.learningRate = 0.0005\n",
        "\n",
        "    self.DQN = self.buildDQN()\n",
        "    self.targetDQN = self.buildDQN()\n",
        "    self.bestDQN = None\n",
        "\n",
        "    \n",
        "\n",
        "  def policy(self,state):\n",
        "    \"\"\"\n",
        "    Takes greedy actions by maximising the Q-function. \n",
        "    Takes random action epsilon % of the time\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    State: np.ndarry\n",
        "      The current state as defined by the action from the prior state\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    Action: np.ndarry\n",
        "      Action to take given the input state\n",
        "    \"\"\"\n",
        "    stateInput = tf.convert_to_tensor(state[None,:],dtype=tf.float32)\n",
        "    if random.random() < self.epsilon:\n",
        "      action = np.random.randint(self.actionSpace)\n",
        "    else:\n",
        "      qActions = self.DQN(stateInput)\n",
        "      action = np.argmax(qActions.numpy()[0],axis=0)\n",
        "    return action\n",
        "\n",
        "  def buildDQN(self):\n",
        "    \"\"\"\n",
        "    Builds the deep Q network\n",
        "\n",
        "    Returns\n",
        "    ------\n",
        "    DQN: <class 'tensorflow.python.keras.engine.sequential.Sequential'>\n",
        "      The compiled model\n",
        "    \"\"\"\n",
        "    DQN = Sequential()    \n",
        "    DQN.add(Dense(128,input_dim=self.stateSpace,activation='relu',kernel_initializer='glorot_uniform'))\n",
        "    DQN.add(Dense(64,activation='relu',kernel_initializer='glorot_uniform'))\n",
        "    DQN.add(Dense(32,activation='relu',kernel_initializer='glorot_uniform'))\n",
        "    DQN.add(Dense(self.actionSpace,activation='linear', kernel_initializer='glorot_uniform'))\n",
        "    DQN.compile(optimizer=Adam(self.learningRate),loss='mse')\n",
        "    return DQN\n",
        "   \n",
        "\n",
        "  def train(self,batch,step):\n",
        "    \"\"\"\n",
        "    A training step\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    batch: list\n",
        "      Batch of 32 frames. Each contains an array of [state,nextState,reward,\n",
        "      action,done]\n",
        "    \n",
        "    step: int\n",
        "      Step of total training steps\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model.history['loss']: <class: list>\n",
        "      Output log of loss over training step\n",
        "    \"\"\"\n",
        "    stateBatch = []\n",
        "    nextStateBatch = []\n",
        "    actionBatch = []\n",
        "    rewardBatch = []\n",
        "    doneBatch = []\n",
        "    \n",
        "    for frame in batch:\n",
        "      stateBatch.append(frame[0])\n",
        "      nextStateBatch.append(frame[1])\n",
        "      rewardBatch.append(frame[2])\n",
        "      actionBatch.append(frame[3])\n",
        "      doneBatch.append(frame[4])\n",
        "\n",
        "    stateBatch = np.array(stateBatch)\n",
        "    nextStateBatch = np.array(nextStateBatch)\n",
        "    actionBatch = np.array(actionBatch)\n",
        "    recentQ = self.DQN(stateBatch)\n",
        "    targetQ = np.copy(recentQ)\n",
        "    nextQ = self.targetDQN(nextStateBatch)\n",
        "    nextQMax= np.amax(nextQ,axis=1)\n",
        "    for i in range(stateBatch.shape[0]):\n",
        "      #discounted reward calculation\n",
        "      targetQ[i][int(actionBatch[i])]= rewardBatch[i] if doneBatch[i] else rewardBatch[i] + self.gamma * nextQMax[i]\n",
        "    \n",
        "    #fit model\n",
        "    model = self.DQN.fit(x=stateBatch,\n",
        "                          y=targetQ,\n",
        "                          verbose=0,\n",
        "                          batch_size=None)\n",
        "    return model.history['loss']\n",
        "\n",
        "\n",
        "  def softUpdate(self):\n",
        "    \"\"\"\n",
        "    Performs a soft update to the targetDQN. The updated is scaled by the TAU\n",
        "    class attribute\n",
        "    \"\"\"\n",
        "    targetModel = self.targetDQN\n",
        "    model = self.DQN\n",
        "    targetParams = np.array(targetModel.get_weights(),dtype=object)\n",
        "    modelParams = np.array(model.get_weights(),dtype=object)\n",
        "    targetModel.set_weights(self.TAU*modelParams + (1.0-self.TAU)*targetParams)\n",
        "    self.targetDQN = targetModel\n",
        "  \n",
        "  \n",
        "  def saveModel(self,modelPath):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "    modelPath: string\n",
        "      Path to save model to\n",
        "\n",
        "    Saves the model to modelPath. Saved as a .json. Weights are saved seperately\n",
        "    \"\"\"\n",
        "    # modelJson = self.bestDQN.to_json()\n",
        "    # with open((modelPath + \"model.json\"), \"w\") as jsonFile:\n",
        "    #     jsonFile.write(modelJson)\n",
        "    # self.bestDQN.save_weights((modelPath + 'DQNWeights.tf'),save_format='tf')\n",
        "\n",
        "    # print('model saved')\n",
        "    pass\n",
        "  def setBestModel(self):\n",
        "    \"\"\"\n",
        "    Sets the best model from online model\n",
        "    \"\"\"\n",
        "    self.bestDQN = self.DQN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faHZbuqSvHvC"
      },
      "source": [
        "class ReplayBuffer():\n",
        "  \"\"\"\n",
        "  Replay buffer class\n",
        "  Saves training data\n",
        "  \n",
        "  Parameters\n",
        "  ----------\n",
        "  self.gameplayExperiences: collections.deque \n",
        "    Gameplay stored in deque that allows for easy use and removal of data\n",
        "  \n",
        "  Methods\n",
        "  -------\n",
        "  storeGamePlay(state,nextState,reward,action,done)\n",
        "    Appends a batch of gameplay to gameplayExperiences\n",
        "  \n",
        "  batchGamePlay()\n",
        "    Takes a batch of size 32 from gameplayExperiences\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    self.gameplayExperiences = deque(maxlen=10000)\n",
        "  \n",
        "  def storeGamePlay(self,state,nextState,reward,action,done):\n",
        "    \"\"\"\n",
        "    Stores a frame of gameplay in gameplayExperiences\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    state: numpy ndarry\n",
        "      The state of the enviornment 8 values in length\n",
        "    nextState: numpy ndarry\n",
        "      The state of the enviornment following the current action\n",
        "    reward: float\n",
        "      Reward for taking the current action in the current state\n",
        "    action: int\n",
        "      Action taken in state\n",
        "    done: boolean\n",
        "      True if epsiode completed\n",
        "      False if the episode is still running\n",
        "    \"\"\"\n",
        "    self.gameplayExperiences.append((state,nextState,reward,action,done))\n",
        "    \n",
        "  \n",
        "  def batchGamePlay(self):\n",
        "    \"\"\"\n",
        "    Takes a batch of size 32 from gameplayExperiences\n",
        "\n",
        "    Returns\n",
        "    ------\n",
        "    batch: list\n",
        "      A batch of gameplay \n",
        "    \"\"\"\n",
        "    batchSize = min(32, len(self.gameplayExperiences))\n",
        "    sampleBatch = random.sample(self.gameplayExperiences, batchSize)\n",
        "    batch = []\n",
        "    for gameplay in sampleBatch:\n",
        "      batch.append(gameplay)\n",
        "    return batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw-n7vDJldtR"
      },
      "source": [
        "def evaluateDQN(env, agent, numberOfEpisodes):\n",
        "  \"\"\"\n",
        "  Takes in the agent. Runs numberOfEpisodes test episodes.\n",
        "  Mean rewards and prediction times are calculated.\n",
        "  Parameters\n",
        "  ----------\n",
        "  numberOfEpisodes: int\n",
        "    Number of episodes to evaluate the agent on\n",
        "  \n",
        "  agent : <class: Evaluator>\n",
        "    Agent to test\n",
        "    \n",
        "  Returns\n",
        "  -------\n",
        "  meanReward: float\n",
        "    Mean reward over the test episodes\n",
        "  \"\"\"\n",
        "  rewardSum = 0.0\n",
        "  for episode in range(numberOfEpisodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    state = None\n",
        "    episodeReward = 0.0\n",
        "    while not done:\n",
        "      action = agent.policy(obs)\n",
        "      newObs, reward, done, _ = env.step(action)\n",
        "      episodeReward += reward\n",
        "      obs = newObs\n",
        "    rewardSum += episodeReward\n",
        "  meanReward = rewardSum / numberOfEpisodes\n",
        "  return meanReward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNzhRYDCxDi9"
      },
      "source": [
        "def collectGamePlay(env,agent,buffer):\n",
        "  \"\"\"\n",
        "  Collects an episode of gameplay\n",
        "  When done the episode is over the function terminates\n",
        "  \"\"\"\n",
        "  state = env.reset()\n",
        "  done=False\n",
        "  while not done:\n",
        "    action = agent.policy(state)\n",
        "    nextState,reward,done,_ = env.step(action)\n",
        "    buffer.storeGamePlay(state,nextState,reward,action,done)\n",
        "    state=nextState\n",
        "\n",
        "def trainModel(numSteps, evaluationInterval):\n",
        "  \"\"\"\n",
        "  Trains the agent over a user defined number of steps\n",
        "  Parameters\n",
        "  ----------\n",
        "  numSteps: int\n",
        "    Number of steps to train model - 100000 recommened\n",
        "  \n",
        "  evaluationInterval: int\n",
        "    Frequency of validation testing\n",
        "  \"\"\"\n",
        "  #Initalise pandas results frame\n",
        "  if numSteps < evaluationInterval:\n",
        "    zeros = np.zeros((1,2))\n",
        "  else:\n",
        "    zeros = np.zeros((int(numSteps/evaluationInterval),2))\n",
        "  rewardData = pd.DataFrame(zeros,columns=['meanReward', 'episodeNumber'])\n",
        "\n",
        "  #Initalise lunar lander environment\n",
        "  env=gym.make('LunarLander-v2')\n",
        "  env.seed(0)\n",
        "  #Find size of state and action space\n",
        "  stateSpace = env.observation_space.shape[0]\n",
        "  actionSpace = env.action_space.n\n",
        "  #Init agent and replay buffer\n",
        "  agent=DQNAgent(actionSpace, stateSpace)\n",
        "  buffer=ReplayBuffer()\n",
        "  #asign iter for index pandas frame\n",
        "  iter = 0\n",
        "  #set best reward low\n",
        "  bestReward=-np.inf\n",
        "  training=True\n",
        "  #paths for model saving\n",
        "  modelPath = '/content/drive/My Drive/Github/RLProject/SavedModels/'\n",
        "  checkpointPath = '/content/drive/My Drive/Github/RLProject/SavedModels/DQNCheckpoint'\n",
        "  while training:\n",
        "    for step in range(numSteps):\n",
        "      if step%1000==0:\n",
        "        print(f'Step num:{step}')\n",
        "      \n",
        "      #Fill buffer and train model\n",
        "      collectGamePlay(env,agent,buffer)\n",
        "      experienceBatchGamePlay=buffer.batchGamePlay()\n",
        "      loss = agent.train(experienceBatchGamePlay,step)\n",
        "      #Perform soft update\n",
        "      agent.softUpdate()\n",
        "      \n",
        "      \n",
        "      \n",
        "      # same callback structure as sb\n",
        "      if step % evaluationInterval == 0:      \n",
        "        meanReward = evaluateDQN(env,agent,5)\n",
        "        rewardData['meanReward'].iloc[iter]=meanReward\n",
        "        rewardData['episodeNumber'].iloc[iter]=step \n",
        "        print('validating')\n",
        "\n",
        "        #Threshold of completion according to open ai gym\n",
        "        #Stop training at 200 reward\n",
        "        if meanReward>=200:\n",
        "          agent.setBestModel()\n",
        "          agent.saveModel(modelPath)\n",
        "\n",
        "          print('Reward 200 reached - training complete')\n",
        "          print('Best model saved')\n",
        "          training = False\n",
        "          break\n",
        "        \n",
        "        \n",
        "        #set best model and save for checkpoint if there is a crash\n",
        "        if meanReward>bestReward:\n",
        "          agent.setBestModel()\n",
        "          bestReward=meanReward\n",
        "          print('New best score')\n",
        "          # rewardData.to_csv('/content/drive/My Drive/Github/RLProject/SavedModels/DQNReward.csv',index=False)\n",
        "        iter+=1\n",
        "      #save for checkpoint if there is a crash\n",
        "      if step%5000 == 0:\n",
        "        agent.saveModel((checkpointPath+f'/Checkpoint@step{step}'))\n",
        "        # rewardData.to_csv('/content/drive/My Drive/Github/RLProject/SavedModels/DQNReward.csv',index=False)\n",
        "    \n",
        "    #training over\n",
        "    training = False\n",
        "\n",
        "  #Save the best model\n",
        "  agent.saveModel(modelPath)\n",
        "  # rewardData.to_csv('/content/drive/My Drive/Github/RLProject/SavedModels/DQNReward.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4uI8fF_3qjn"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  trainModel(100000,500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1jQdMcX1VxJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}