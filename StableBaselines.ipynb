{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "StableBaselines.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QlgX_E7JC6l"
      },
      "source": [
        "# 0. Install and Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktM_sMpRIWF5"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nasvqa12I0sp"
      },
      "source": [
        "%cd /content/drive/My Drive/Github/RLProject/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "6_2tMYUYJC6y"
      },
      "source": [
        "# %tensorflow_version 1.x\n",
        "# !pip install tensorflow==1.15.0 tensorflow-gpu==1.15.0 stable_baselines gym box2d-py --user\n",
        "# %tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWskDE2c9WoN"
      },
      "source": [
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x;\n",
        "!apt-get install ffmpeg freeglut3-dev xvfb;  # For visualization\n",
        "!pip install stable-baselines[mpi]==2.10.2;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t19Sq2_ta6Pq"
      },
      "source": [
        "# !pip install swig\n",
        "!pip install box2d-py;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U29X1-B-AIKE"
      },
      "source": [
        "import stable_baselines\n",
        "stable_baselines.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDNBG1BaJC61"
      },
      "source": [
        "import gym \n",
        "from stable_baselines import A2C,PPO2,DQN\n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "from stable_baselines.common.evaluation import evaluate_policy\n",
        "from stable_baselines.common.callbacks import EvalCallback,  StopTrainingOnRewardThreshold\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDLOzh35FKHC"
      },
      "source": [
        "%cd /content/drive/My Drive/Github/RLProject/DQNModelData"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bK0RPUVvAKWR"
      },
      "source": [
        "def findHypParams():\n",
        "  \"\"\"\n",
        "  Finds the optimal hyper-parameter configuration for each model by looping through\n",
        "  parameter dictionaries. Returns the optimal models and writes the parameters to \n",
        "  a file.\n",
        "\n",
        "  Returns\n",
        "  ------- \n",
        "  optimalA2CModel: <class 'stable_baselines.a2c.a2c.A2C'>\n",
        "\n",
        "  optimalPPO2Model: <class 'stable_baselines.ppo2.ppo2.PPO2'>\n",
        "\n",
        "  optimalDQNModel: <class 'stable_baselines.deepq.dqn.DQN'>\n",
        "  \"\"\"\n",
        "  env = gym.make('LunarLander-v2')\n",
        "  env = DummyVecEnv([lambda: env])\n",
        "  PPO2ParamDict = {'gamma':(0.99,0.95),\n",
        "                 'n_steps':(128,132),\n",
        "                 'ent_coef':(0.01,0.02),\n",
        "                 'learning_rate':(0.0005,0.00025)\n",
        "  }\n",
        "  A2CParamDict = {'gamma':(0.99,0.95),\n",
        "                  'n_steps':(5,7),\n",
        "                  'ent_coef':(0.01,0.02),  \n",
        "                  'learning_rate':(0.0007,0.0005)\n",
        "      \n",
        "  }\n",
        "  DQNParamDict = {'gamma':(0.99,0.95),\n",
        "                  'double_q':(True,False),\n",
        "                  'batch_size':(32,64),  \n",
        "                  'learning_rate':(0.0007,0.0005)\n",
        "      \n",
        "  }\n",
        "\n",
        "  bestReward=-np.inf\n",
        "  iter=0\n",
        "  keys = list(DQNParamDict.keys())\n",
        "  for gamma in DQNParamDict[keys[0]]:\n",
        "    for q in DQNParamDict[keys[1]]:\n",
        "      for batch in DQNParamDict[keys[2]]:\n",
        "        for lr in DQNParamDict[keys[3]]:\n",
        "          iter+=1\n",
        "          print(iter)\n",
        "          model = DQN('MlpPolicy', \n",
        "                        env, \n",
        "                        verbose = 0,\n",
        "                        tensorboard_log=\"./compareModels_tensorboard/\",\n",
        "                        seed=42,\n",
        "                        gamma=gamma,\n",
        "                        double_q=q,\n",
        "                        batch_size=batch,\n",
        "                        learning_rate=lr)\n",
        "          model.learn(total_timesteps=1000)\n",
        "          meanReward, stdReward  = evaluate_policy(model,\n",
        "                                                  env,\n",
        "                                                  n_eval_episodes=100,\n",
        "                                                  render=False)\n",
        "          if meanReward>bestReward:\n",
        "            bestReward=meanReward\n",
        "            optimalDQNParams=[gamma,q,batch,lr]\n",
        "            optimalDQNModel = model\n",
        "\n",
        "    \n",
        "    with open('optimalDQNParams.txt', 'w') as f:\n",
        "      for item in optimalDQNParams:\n",
        "        f.write(\"%s\\n\" % item)\n",
        "\n",
        "\n",
        "\n",
        "  bestReward=-np.inf\n",
        "  iter=0\n",
        "  keys = list(PPO2ParamDict.keys())\n",
        "  for gamma in PPO2ParamDict[keys[0]]:\n",
        "   for steps in PPO2ParamDict[keys[1]]:\n",
        "     for ent in PPO2ParamDict[keys[2]]:\n",
        "       for lr in PPO2ParamDict[keys[3]]:\n",
        "         iter+=1\n",
        "         print(iter)\n",
        "         model = PPO2('MlpPolicy', \n",
        "                      env, \n",
        "                      verbose = 0,\n",
        "                      tensorboard_log=\"./compareModels_tensorboard/\",\n",
        "                      seed=42,\n",
        "                      gamma=gamma,\n",
        "                      n_steps=steps,\n",
        "                      ent_coef=ent,\n",
        "                      learning_rate=lr)\n",
        "         model.learn(total_timesteps=10000)\n",
        "         meanReward, stdReward  = evaluate_policy(model,\n",
        "                                                 env,\n",
        "                                                 n_eval_episodes=100,\n",
        "                                                 render=False)\n",
        "         if meanReward>bestReward:\n",
        "           bestReward=meanReward\n",
        "           optimalPPO2Params=[gamma,steps,ent,lr]\n",
        "           optimalPPO2Model = model\n",
        "\n",
        "   \n",
        "  with open('optimalPPO2Params.txt', 'w') as f:\n",
        "    for item in optimalPPO2Params:\n",
        "      f.write(\"%s\\n\" % item)\n",
        "  bestReward=-100000\n",
        "  keys = list(A2CParamDict.keys())\n",
        "  for gamma in A2CParamDict[keys[0]]:\n",
        "    for steps in A2CParamDict[keys[1]]:\n",
        "      for ent in A2CParamDict[keys[2]]:\n",
        "        for lr in A2CParamDict[keys[3]]:\n",
        "          iter+=1\n",
        "          print(iter)\n",
        "          model = A2C('MlpPolicy', \n",
        "                      env, \n",
        "                      verbose = 0,\n",
        "                      tensorboard_log=\"./compareModels_tensorboard/\",\n",
        "                      seed=42,\n",
        "                      gamma=gamma,\n",
        "                      n_steps=steps,\n",
        "                      ent_coef=ent,\n",
        "                      learning_rate=lr)\n",
        "          model.learn(total_timesteps=10000)\n",
        "          meanReward, stdReward  = evaluate_policy(model,\n",
        "                                                 env,\n",
        "                                                 n_eval_episodes=100,\n",
        "                                                 render=False)\n",
        "          if meanReward>bestReward:\n",
        "            bestReward=meanReward\n",
        "            optimalA2CParams=[gamma,steps,ent,lr]\n",
        "            optimalA2CModel = model\n",
        "\n",
        "\n",
        "\n",
        "  with open('optimalA2CParams.txt', 'w') as f:\n",
        "      for item in optimalA2CParams:\n",
        "        f.write(\"%s\\n\" % item)\n",
        "  \n",
        "  return optimalA2CModel,optimalPPO2Model,optimalDQNModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD9zgnlJKzzs"
      },
      "source": [
        "# %cd DQNModelData\n",
        "# optimalA2CModel,optimalPPO2Model,optimalDQNModel=findHypParams()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq05_Ik6jBPF"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6CTsOMrnU4w"
      },
      "source": [
        "%cd /content/drive/My Drive/Github/RLProject/SavedModels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0ILgxe5m0kg"
      },
      "source": [
        "#All models are trained for 100000 timesteps\n",
        "#All use same callback system"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnJuugOwjWqv"
      },
      "source": [
        "#Path to save checkpoints and validation data\n",
        "logPath = '/content/drive/My Drive/Github/RLProject/SavedModels/A2CLogs/'\n",
        "eval_env = gym.make('LunarLander-v2')\n",
        "#Stops training when envrionment is 'completed'\n",
        "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=200, verbose=1)\n",
        "#Saves training and validation data\n",
        "eval_callback = EvalCallback(eval_env, best_model_save_path=logPath,\n",
        "                             callback_on_new_best=callback_on_best,\n",
        "                             log_path=logPath, eval_freq=500,\n",
        "                             deterministic=True, render=False,\n",
        "                             verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhcLIROejDPK"
      },
      "source": [
        "#Initalise environment \n",
        "env = gym.make('LunarLander-v2')\n",
        "#OpenAI Gym recommends using the dummy vec env\n",
        "env = DummyVecEnv([lambda: env])\n",
        "A2CModel = A2C('MlpPolicy', \n",
        "                      env, \n",
        "                      verbose = 0,\n",
        "                      tensorboard_log=\"./lunarlander_tensorboard/\",\n",
        "                      seed=42,\n",
        "                      gamma=0.95,\n",
        "                      n_steps=5,\n",
        "                      ent_coef=0.01,\n",
        "                      learning_rate=0.0007)\n",
        "A2CModel.learn(total_timesteps=100000,\n",
        "            callback=eval_callback)\n",
        "modelPath = '/content/drive/My Drive/Github/RLProject/SavedModels/'\n",
        "#Save trained model\n",
        "A2CModel.save((modelPath + 'finalA2CModel'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGPoBf3ZjRPx"
      },
      "source": [
        "logPath = '/content/drive/My Drive/Github/RLProject/SavedModels/PPO2Logs/'\n",
        "eval_env = gym.make('LunarLander-v2')\n",
        "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=200, verbose=1)\n",
        "eval_callback = EvalCallback(eval_env, best_model_save_path=logPath,\n",
        "                             callback_on_new_best=callback_on_best,\n",
        "                             log_path=logPath, eval_freq=500,\n",
        "                             deterministic=True, render=False,\n",
        "                             verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPLK8VznmfiR"
      },
      "source": [
        "env = gym.make('LunarLander-v2')\n",
        "env = DummyVecEnv([lambda: env])\n",
        "PPO2Model = PPO2('MlpPolicy', \n",
        "                      env, \n",
        "                      verbose = 0,\n",
        "                      tensorboard_log=\"./lunarlander_tensorboard/\",\n",
        "                      seed=42,\n",
        "                      gamma=0.99,\n",
        "                      n_steps=132,\n",
        "                      ent_coef=0.02,\n",
        "                      learning_rate=0.00025)\n",
        "PPO2Model.learn(total_timesteps=100000,\n",
        "            callback=eval_callback)\n",
        "modelPath = '/content/drive/My Drive/Github/RLProject/SavedModels/'\n",
        "PPO2Model.save((modelPath + 'finalPPO2Model'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRFQksN26DFQ"
      },
      "source": [
        "logPath = '/content/drive/My Drive/Github/RLProject/SavedModels/DQNLogs/'\n",
        "eval_env = gym.make('LunarLander-v2')\n",
        "callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=200, verbose=1)\n",
        "eval_callback = EvalCallback(eval_env, best_model_save_path=logPath,\n",
        "                             callback_on_new_best=callback_on_best,\n",
        "                             log_path=logPath, eval_freq=500,\n",
        "                             deterministic=True, render=False,\n",
        "                             verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzLjOttYFeD2"
      },
      "source": [
        "env = gym.make('LunarLander-v2')\n",
        "env = DummyVecEnv([lambda: env])\n",
        "DQNModel = DQN('MlpPolicy', \n",
        "                      env, \n",
        "                      verbose = 0,\n",
        "                      tensorboard_log=\"./lunarlander_tensorboard/\",\n",
        "                      seed=42,\n",
        "                      gamma=0.99,\n",
        "                      double_q=True,\n",
        "                      batch_size=32,\n",
        "                      learning_rate=0.0007)\n",
        "DQNModel.learn(total_timesteps=100000,\n",
        "            callback=eval_callback)\n",
        "modelPath = '/content/drive/My Drive/Github/RLProject/SavedModels/'\n",
        "DQNModel.save((modelPath + 'finalDQNModel'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}