{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experiment.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktM_sMpRIWF5"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')#, force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-Quq2Xr5nre"
      },
      "source": [
        "%cd /content/drive/My Drive/Github/RLProject/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cc94M9r78CYB"
      },
      "source": [
        "#Tensorflow 1.x required for stable baselines\n",
        "%tensorflow_version 1.x\n",
        "!apt-get install ffmpeg freeglut3-dev xvfb;  # For visualization\n",
        "!pip install stable-baselines[mpi]==2.10.2;\n",
        "#OpenAI Gym package that includes LunarLander-V2\n",
        "!pip install box2d-py;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTz0IyLtzmF9"
      },
      "source": [
        "import stable_baselines\n",
        "stable_baselines.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEH7bq8KP_rV"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import gym \n",
        "import os, sys\n",
        "\n",
        "from stable_baselines import A2C,PPO2,DQN\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import model_from_json\n",
        "\n",
        "import pandas as pd\n",
        "from dataclasses import make_dataclass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hk5QNr8Yl1Ft"
      },
      "source": [
        "class Evaluator():\n",
        "  \"\"\"\n",
        "  Evaluator class\n",
        "\n",
        "  Attributes\n",
        "\n",
        "  ----------\n",
        "  homemadeDQN: <class 'tensorflow.python.keras.engine.sequential.Sequential'>\n",
        "    Imported trained Keras DQN model\n",
        "\n",
        "  A2C: <class 'stable_baselines.a2c.a2c.A2C'>\n",
        "    Imported trained Stable Baselines (SB) A2C model\n",
        "\n",
        "  PPO2: <class 'stable_baselines.ppo2.ppo2.PPO2'>\n",
        "    Imported trained Stable Baselines (SB) PPO2 model\n",
        "\n",
        "  DQN: <class 'stable_baselines.deepq.dqn.DQN'>\n",
        "    Imported trained Stable Baselines (SB) DQN model\n",
        "\n",
        "  env: <class 'gym.wrappers.time_limit.TimeLimit'>\n",
        "    LunarLander-V2 OpenAI Gym environment\n",
        "\n",
        "  testEpisodes: int\n",
        "    Number of episodes to test each model for\n",
        "\n",
        "  Methods\n",
        "\n",
        "  -------\n",
        "\n",
        "  loadModels()\n",
        "    Loads the trained models from the model paths and asigns them to a class\n",
        "    variable.\n",
        "\n",
        "  testModels(model)\n",
        "    Calculates mean reward and mean prediction time over a number of episodes\n",
        "\n",
        "  comparePerformance(testEpisodes)\n",
        "    Runs testModels() for each model. Creates results dataframe.\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    ####perhaps laod in paths instead\n",
        "    self.homemadeDQN = None\n",
        "    self.A2C = None\n",
        "    self.PPO2 = None\n",
        "    self.DQN = None\n",
        "    self.testEpisodes = 10\n",
        "    # self.testEpisodes = testEpisodes\n",
        "    self.env = gym.make('LunarLander-v2')\n",
        "    self.env.seed(42)\n",
        "\n",
        "  def loadModels(self):\n",
        "    \"\"\"\n",
        "    ----------\n",
        "\n",
        "    SB models are loaded with a load function unique to each model \n",
        "    class.\n",
        "    The Keras model is saved as a .json with weights saved separately. The model\n",
        "    is first loaded then weights are set. The model.load Tensorflow function \n",
        "    is for tf.2 which is not usable with SB models\n",
        "    \"\"\"\n",
        "    #Set to your path if there are issues\n",
        "    path = str(os.getcwd())\n",
        "    modelPath = path + '/SavedModels/'\n",
        "    self.A2C = A2C.load(modelPath + 'finalA2CModel')\n",
        "    self.PPO2 = PPO2.load(modelPath + 'finalPPO2Model')\n",
        "    self.DQN = DQN.load(modelPath + 'finalDQNModel')\n",
        "    # load DQN from json\n",
        "    jsonFile = open((modelPath + 'model.json'), 'r')\n",
        "    DQNjson = jsonFile.read()\n",
        "    jsonFile.close()\n",
        "    loadedDQN = model_from_json(DQNjson)\n",
        "    # load weights into new model\n",
        "    loadedDQN.load_weights((modelPath + \"DQNWeights.tf\"))\n",
        "    self.homemadeDQN = loadedDQN\n",
        "\n",
        "  def testModel(self,model):\n",
        "    \"\"\"\n",
        "    Takes in a class attribuate model. Runs testEpisodes number of test episodes.\n",
        "    Mean rewards and prediction times are calculated.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    meanReward: float\n",
        "      Mean reward over the test episodes\n",
        "  \n",
        "    meanPredictionTime: float\n",
        "      Mean time taken to make a prediciton\n",
        "    \"\"\"\n",
        "    rewardSum = 0.0\n",
        "    runs = 0\n",
        "    predictionTimeSum = 0.0\n",
        "    for episode in range(self.testEpisodes):\n",
        "      obs = self.env.reset()\n",
        "\n",
        "      state = None\n",
        "      done = False\n",
        "      episodeReward = 0.0\n",
        "      while not done:\n",
        "        obs = np.reshape(obs,(1,obs.shape[0]))\n",
        "        #runs prediction for stable baselines models (A2C,PPO2,DQN)\n",
        "        if \"stable\" in str(type(model)):\n",
        "          runs+=1\n",
        "          t1 = time.perf_counter()      \n",
        "          action, state = model.predict(obs)\n",
        "          newObs, reward, done, _info = self.env.step(action[0])\n",
        "          predictionTime = time.perf_counter()-t1\n",
        "        #runs prediction for tensorflow DQN model\n",
        "        else:\n",
        "          runs+=1\n",
        "          t1 = time.perf_counter()\n",
        "          actions = model.predict(obs)\n",
        "          action = np.argmax(actions)\n",
        "          newObs, reward, done, _ = self.env.step(action)\n",
        "          predictionTime = time.perf_counter()-t1\n",
        "        obs = newObs\n",
        "        predictionTimeSum += predictionTime\n",
        "        episodeReward += reward\n",
        "      rewardSum += episodeReward\n",
        "    meanReward = rewardSum / self.testEpisodes\n",
        "    meanPredictionTime = predictionTimeSum/runs\n",
        "    return meanReward,meanPredictionTime\n",
        "\n",
        "  def comparePerformance(self, testEpisodes):\n",
        "    \"\"\"\n",
        "    Compares the performance of each model by calling testModel and creating a \n",
        "    pandas dataframe of the results\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    testEpisodes: int\n",
        "      Number of episodes to test the model on\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    resultsFrame: pandas dataframe\n",
        "      Dataframe that includes the results\n",
        "    \"\"\"\n",
        "    self.testEpisodes = testEpisodes\n",
        "    #Load the models\n",
        "    self.loadModels()\n",
        "    data=[]\n",
        "    #Compile new point class for experimental results\n",
        "    Point = make_dataclass(\"Point\", [(\"meanRewards\", int),('meanPredictionTime',float), (\"models\", str)])\n",
        "    #Test A2C\n",
        "    meanReward,meanPredictionTime = self.testModel(self.A2C)\n",
        "    data.append(Point(meanReward,meanPredictionTime,'A2C'))\n",
        "    #Test PPO2\n",
        "    meanReward,meanPredictionTime = self.testModel(self.PPO2)\n",
        "    data.append(Point(meanReward,meanPredictionTime,'PPO2'))\n",
        "    #Test DQN\n",
        "    meanReward,meanPredictionTime = self.testModel(self.DQN)\n",
        "    data.append(Point(meanReward,meanPredictionTime,'DQN'))\n",
        "    #Test homemadeDQN\n",
        "    meanReward,meanPredictionTime = self.testModel(self.homemadeDQN)\n",
        "    data.append(Point(meanReward,meanPredictionTime,'homemadeDQN'))\n",
        "    \n",
        "    #Create panads results frame\n",
        "    resultFrame = pd.DataFrame(data=data,columns=['meanRewards','meanPredictionTime','models'])\n",
        "    return resultFrame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M7OSABQIWlT"
      },
      "source": [
        "if __name__=='__main__':\n",
        "  evaluator = Evaluator()\n",
        "  z = evaluator.comparePerformance(testEpisodes=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNNaGSWua579"
      },
      "source": [
        "# Plotting training scores\n",
        "## Don't run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMyWdG6v6S9n"
      },
      "source": [
        "# plt.rc('font',size=14)\n",
        "# plt.rcParams['font.family'] = 'serif'\n",
        "# plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\n",
        "# x=np.load(\"/content/drive/My Drive/Github/RLProject/SavedModels/PPO2Logs/evaluations.npz\")\n",
        "# results = x['results'].squeeze()\n",
        "# meanResultsPPO = x['results'].squeeze().mean(axis=1)\n",
        "# timestepsPPO=x['timesteps']\n",
        "# x=np.load(\"/content/drive/My Drive/Github/RLProject/SavedModels/A2CLogs/evaluations.npz\")\n",
        "# results = x['results'].squeeze()\n",
        "# meanResultsA2C = x['results'].squeeze().mean(axis=1)\n",
        "# timestepsA2C=x['timesteps']\n",
        "# x=np.load(\"/content/drive/My Drive/Github/RLProject/SavedModels/DQNLogs/evaluations.npz\")\n",
        "# results = x['results'].squeeze()\n",
        "# meanResultsDQN = x['results'].squeeze().mean(axis=1)\n",
        "# timestepsDQN=x['timesteps']\n",
        "# KerasDQNData = pd.read_csv('/content/drive/My Drive/Github/RLProject/SavedModels/DQNReward.csv')\n",
        "# KerasDQNData = KerasDQNData.iloc[0:30]\n",
        "# meanResultsKerasDQN = KerasDQNData['meanReward']\n",
        "# timestepsKerasDQN = KerasDQNData['episodeNumber']\n",
        "# A2Cline,=plt.plot(timestepsA2C,np.log(meanResultsA2C),label='A2C')\n",
        "# PPOline,=plt.plot(timestepsPPO,np.log(meanResultsPPO),label='PPO2')\n",
        "# DQNline,=plt.plot(timestepsDQN,np.log(meanResultsDQN),label='DQN')\n",
        "# KerasDQNline,=plt.plot(timestepsKerasDQN,np.log(meanResultsKerasDQN),label='KerasDQN')\n",
        "# # plt.yscale('log')\n",
        "# plt.legend(handles=[A2Cline, DQNline, PPOline, KerasDQNline])\n",
        "# plt.xlabel('Steps')\n",
        "# plt.ylabel('Mean reward on validation \\n test')\n",
        "# # plt.savefig('/content/drive/My Drive/Github/RLProject/SavedModels/trainingresults.png', bbox_inches=\"tight\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}